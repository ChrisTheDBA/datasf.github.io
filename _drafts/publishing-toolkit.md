---
title: Publishing Toolkit
author:
image:
  thumb:
  feature:
  caption:
  source:
    title:
    author:
    source_link:
    author_link:
    license:
    license_link:
description:
date:
categories:
  - blog
---


How do you operationalize open data? It's a question that comes to us quite a bit from peer programs and researchers. One that we can readily answer, but it can be a lot to absorb in a single conversation.

We've been iterating and improving on how we run the program since Joy arrived as our City's first Chief Data Officer. Lots of collateral has been developed over time, but we've wanted to thread all of the work together into a sort of "operating manual" for open data.

We were inspired by Chicago's ETL toolkit, but wanted to encapsulate the mix of people, process and technology we use to operate on a daily basis.

The result is the DataSF Open Data Publishing Toolkit.

It is meant to be an enduring resource for our team (one that will update as we change) as well as a reference for others grappling with

By no means do we believe this is THE way. Your context will differ, your needs will evolve (so will ours). But we welcome you to plunder the toolkit and take what works for you. No matter what you take away, we do have a set of principles that contribute to running a flexible program, and we don't want you to miss them, so I'll call them out here.

## Leverage lightly coupled systems rather than monolithic solutions.

There is no artisinal off the shelf technology for running an open data program soup to nuts. The market is just not big enough and anyone telling you that's what they sell is probably stretching the truth.

We opted for an integration approach rather than waiting for a magical system (that will likely never come). It started as a simple decision to assign unique identifiers to each dataset in our inventory, and now we can link multiple pieces together without thinking too hard.

## Design processes and systems that meet people where they are.

We work with 52 data coordinators across a highly variable IT environment. It is impossible to design the "perfect" system. But what you can do is listen to the users of your systems and iterate.

For example, our initial publishing intake was in an Excel workbook. It was our best effort at designing something without too much investment. It had macros and automated "all the things." And it worked, kind of. We learned a lot and lifted those learnings into an online form that now serves as our intake.

## Reduce duplication of effort.

Notice, I say reduce. It is a high bar to completely avoid duplication, but you can always be scanning for ways to remove duplicative entry. It's this principle that has led to us pushing for a metadata API that Socrata just released so we can collect data once through intake and load that data automatically.

Orient toward scaling where feasible.

Continuously evaluate and improve.

There ARE technologies for pieces of managing a program. There are of course open data portals, there are help desk ticketing systems, there are kanban boards, etc. But the arrangement of your program may vary